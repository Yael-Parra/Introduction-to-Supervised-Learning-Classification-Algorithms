{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d2ac5a",
   "metadata": {},
   "source": [
    "# <span style=\"color:#f6f794\"> Classification Algorithms </span> ðŸ“š "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b5388",
   "metadata": {},
   "source": [
    "## <span style=\"color:#c69005\"> 1. How they work and how they differ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1814458",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.1 **Logistic Regression** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concept**</span>\n",
    "\n",
    "Despite its name, logistic regression is a classification algorithm that estimates the probability of an instance belonging to a particular class.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**How it works**</span>\n",
    "1. Calculates a linear combination of features (similar to linear regression)\n",
    "2. Applies the logistic function (sigmoid) to transform the result into a probability between 0 and 1\n",
    "3. Assigns the class according to a threshold (typically 0.5)\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Mathematical foundations**</span>\n",
    "  - **Logistic function**:\n",
    "  $$\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\text{where } z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$$\n",
    "  - **Probability**:  \n",
    "    $$p(y=1 \\mid \\mathbf{x}) = \\sigma(z)$$\n",
    "  - **Cost function** (log-loss or binary cross-entropy):  \n",
    "    $$J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]$$\n",
    "  - **Optimization**:  \n",
    "    Typically via **gradient descent**:\n",
    "    $$\\beta_j := \\beta_j - \\alpha \\frac{\\partial J(\\beta)}{\\partial \\beta_j}$$\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Advantages**</span>\n",
    "- Provides probabilities, not just class predictions\n",
    "- Relatively simple and efficient\n",
    "- Coefficients are interpretable as log-odds\n",
    "- Works well with large datasets\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitations**</span>\n",
    "- Assumes linear relationship between variables and the logarithm of odds\n",
    "- Does not handle non-linear relationships well without feature transformation\n",
    "- May have problems with imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a92fa",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.2 **K Nearest Neighbors** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concept**</span>\n",
    "\n",
    "KNN is a simple but effective algorithm that classifies a new instance according to the majority class of its k nearest neighbors in the feature space.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**How it works**</span>\n",
    "1. Stores all training data\n",
    "2. For a new instance:\n",
    "   - Calculates the distance to all training instances\n",
    "   - Identifies the k nearest neighbors\n",
    "   - Assigns the class by majority vote among these neighbors\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Mathematical foundations**</span>\n",
    "- Typically uses **Euclidean distance**:  \n",
    "    $$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum (x_i - y_i)^2}$$\n",
    "- Other metrics include Manhattan distance, Minkowski or cosine similarity.\n",
    "- The prediction is:  \n",
    "    $$\\hat{y} = \\arg\\max \\left( \\sum \\mathbb{I}(y_i = c) \\right)$$  \n",
    "    where \\( \\mathbb{I} \\) is an indicator function.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Advantages**</span>\n",
    "- Simple and easy to understand\n",
    "- Makes no assumptions about data distribution\n",
    "- Works well with well-separated classes\n",
    "- Naturally handles multiclass classification\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitations**</span>\n",
    "- Computationally intensive for large datasets\n",
    "- Sensitive to irrelevant features and scale\n",
    "- Requires careful selection of parameter k\n",
    "- Does not produce an explicit model (\"lazy\" learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352eca28",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.3 **SVM (Support Vector Machines)** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concept**</span>\n",
    "\n",
    "SVM seeks to find the optimal hyperplane that maximizes the margin between classes in the feature space, potentially transformed by a kernel function.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**How it works**</span>\n",
    "1. Finds the hyperplane that maximizes the distance between classes\n",
    "2. The points closest to the hyperplane are called \"support vectors\"\n",
    "3. For non-linearly separable problems:\n",
    "   - Uses a kernel function to map the data to a higher-dimensional space\n",
    "   - Finds a separating hyperplane in that space\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Mathematical foundations**</span>\n",
    "- Seeks to maximize the margin:  \n",
    "  $$\\frac{2}{\\|\\mathbf{w}\\|}$$\n",
    "- With constraint:  \n",
    "  $$y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for all points}$$\n",
    "- Parameter \\( C \\) controls the balance between maximizing the margin and minimizing error\n",
    "- Common kernels: linear, polynomial, RBF (radial basis function), sigmoid\n",
    "\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Advantages**</span>\n",
    "- Effective in high-dimensional spaces\n",
    "- Robust when there is a clear separation between classes\n",
    "- Uses only a subset of points (support vectors)\n",
    "- Versatile through different kernel functions\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitations**</span>\n",
    "- Critical selection of parameters and kernel\n",
    "- Not scalable to large datasets\n",
    "- Does not directly provide probabilities\n",
    "- Sensitive to noise and overfitting with certain kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae864c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.4 **Decision Trees for classification** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concept**</span>\n",
    "\n",
    "Decision trees for classification create a model with a tree structure where each internal node represents a test on a feature, each branch represents the outcome of that test, and each leaf represents a class.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**How it works**</span>\n",
    "1. Starts with all data in the root node\n",
    "2. For each potential split:\n",
    "   - Calculates a measure of impurity (Gini or entropy)\n",
    "   - Selects the split that maximizes the reduction in impurity\n",
    "3. Repeats recursively on the child nodes until reaching a stopping criterion\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Mathematical foundations**</span>\n",
    "- **Gini Index**: Measures the probability of incorrectly classifying an element  \n",
    "  $$\\text{Gini} = 1 - \\sum p_i^2$$  \n",
    "  where \\( p_i \\) is the proportion of class \\( i \\)\n",
    "- **Entropy**: Measures uncertainty or disorder in the data  \n",
    "  $$\\text{Entropy} = -\\sum p_i \\log_2(p_i)$$\n",
    "- **Information Gain**:  \n",
    "  $$IG = \\text{Entropy}_{\\text{Parent}} - \\sum \\left( \\frac{n_j}{n} \\cdot \\text{Entropy}_{\\text{Child}_j} \\right)$$\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Advantages**</span>\n",
    "- Easy to understand and interpret\n",
    "- Requires little data preparation\n",
    "- Naturally handles categorical and numerical variables\n",
    "- Can capture non-linear relationships\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitations**</span>\n",
    "- Prone to overfitting, especially with deep trees\n",
    "- Unstable (small changes in data can generate very different trees)\n",
    "- Biased toward features with many categories\n",
    "- Difficulty capturing simple linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424ec94",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.5 **Random Forest for classification** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concept**</span>\n",
    "\n",
    "Random Forest for classification is an ensemble algorithm that combines multiple decision trees to create a more robust and accurate model.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**How it works**</span>\n",
    "1. Creates multiple decision trees using bootstrap aggregating (bagging):\n",
    "   - For each tree, selects a random sample with replacement from the original dataset\n",
    "   - For each split, considers only a random subset of features\n",
    "2. The predicted class is the mode (majority vote) of the classes predicted by the individual trees\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Mathematical foundations**</span>\n",
    "- Diversity among trees is achieved through:\n",
    "  - Bootstrap sampling: Each tree is trained with a different sample\n",
    "  - Feature randomness: At each node, only a random subset of features is considered\n",
    "- For m features, typically âˆšm features are selected at each split for classification\n",
    "- The probability of each class can be estimated as the proportion of trees that voted for that class\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Advantages**</span>\n",
    "- Higher accuracy than individual trees\n",
    "- Significantly reduces overfitting\n",
    "- Robust to outliers and noise\n",
    "- Provides feature importance measures\n",
    "- Works well without much hyperparameter tuning\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitations**</span>\n",
    "- Less interpretable than an individual tree\n",
    "- Computationally intensive\n",
    "- Can be slow for real-time predictions with many trees\n",
    "- Still sensitive to class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35dee0",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.6 **Gradient Boosting for classification** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concept**</span>\n",
    "\n",
    "Gradient Boosting is an ensemble algorithm that builds models sequentially, where each new model attempts to correct the errors of the previous set of models.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**How it works**</span>\n",
    "1. Starts with a simple model (usually a shallow tree)\n",
    "2. Calculates the residual errors of this model\n",
    "3. Trains a new model to predict these residuals\n",
    "4. Adds this new model to the ensemble with a determined weight\n",
    "5. Repeats steps 2-4 until reaching a specific number of models or until errors don't improve significantly\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Mathematical foundations**</span>\n",
    "- Optimizes a loss function (such as cross-entropy for classification)\n",
    "- Uses gradient descent to minimize this function\n",
    "- Each new model is fitted to reduce the negative gradient of the loss function\n",
    "- Applies a learning rate to control the contribution of each model\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Important variants**</span>\n",
    "- **XGBoost**: Optimized and efficient implementation that includes regularization\n",
    "- **LightGBM**: Focus on efficiency with histogram-based splitting\n",
    "- **CatBoost**: Handles categorical variables well and reduces overfitting\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Advantages**</span>\n",
    "- Generally offers better accuracy than Random Forest\n",
    "- Handles different data types well\n",
    "- Provides feature importance measures\n",
    "- Versatile for different loss functions\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitations**</span>\n",
    "- Can overfit if parameters are not well configured\n",
    "- More sensitive to outliers than Random Forest\n",
    "- More parameters to tune\n",
    "- Sequential by nature (difficult to parallelize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95897ff9",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.7 **Neural Networks for classification** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concept**</span>\n",
    "\n",
    "Neural networks are models inspired by the structure of the human brain, composed of layers of interconnected neurons that can learn complex representations of data.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**How it works**</span>\n",
    "1. The network consists of:\n",
    "   - Input layer: Receives the features\n",
    "   - Hidden layers: Transform the data through activation functions\n",
    "   - Output layer: Produces the probabilities for each class\n",
    "2. During training:\n",
    "   - Forward pass: Data passes through the network to obtain predictions\n",
    "   - Error calculation: Predictions are compared with actual labels\n",
    "   - Backpropagation: Weights are updated to minimize error\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Mathematical foundations**</span>\n",
    "- Each neuron applies a linear transformation followed by a non-linear activation function\n",
    "- Common activation functions include ReLU, sigmoid, and tanh\n",
    "- For multiclass classification, the output layer uses the softmax function\n",
    "- The typical loss function is cross-entropy\n",
    "- Weights are updated through gradient descent and backpropagation\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Advantages**</span>\n",
    "- Can capture extremely complex and non-linear relationships\n",
    "- Scalable to large datasets\n",
    "- Excellent for problems where features have hierarchies or patterns\n",
    "- Versatile for different types of data (images, text, time series)\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitations**</span>\n",
    "- Requires large amounts of data to generalize well\n",
    "- Computationally intensive to train\n",
    "- Difficult to interpret (\"black box\")\n",
    "- Many hyperparameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f681d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.8 **Naive Bayes** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concept**</span>\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem with the \"naive\" assumption that features are independent of each other given the class value.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**How it works**</span>\n",
    "1. Uses **Bayes' theorem**:  \n",
    "   $$P(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot P(Y)}{P(X)}$$\n",
    "2. Assumes **conditional independence between features**:  \n",
    "   $$P(X \\mid Y) = P(X_1 \\mid Y) \\cdot P(X_2 \\mid Y) \\cdot \\dots \\cdot P(X_n \\mid Y)$$\n",
    "3. Classifies by choosing the class with the **highest posterior probability**\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Mathematical foundations**</span>\n",
    "- **Bayes' theorem**:  \n",
    "  $$P(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot P(Y)}{P(X)}$$\n",
    "- The predicted class is:  \n",
    "  $$y = \\arg\\max_y \\left[ P(y) \\cdot \\prod P(x_i \\mid y) \\right]$$\n",
    "- \\( P(X) \\) is constant for all classes, so it can be omitted in the maximization\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Main variants**</span>\n",
    "- **Gaussian**: Assumes feature values follow a normal distribution\n",
    "- **Multinomial**: Suitable for count data (frequencies)\n",
    "- **Bernoulli**: For binary/boolean features\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Advantages**</span>\n",
    "- Simple and fast to train\n",
    "- Works well with small datasets\n",
    "- Efficient with high dimensionality\n",
    "- Handles multiclass classification well\n",
    "- Not sensitive to irrelevant noise\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitations**</span>\n",
    "- The independence assumption is rarely true in practice\n",
    "- Can be outperformed by more sophisticated models when there's sufficient data\n",
    "- Does not capture interactions between features\n",
    "- May have problems with continuous features if they don't follow the assumed distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10004c",
   "metadata": {},
   "source": [
    "## <span style=\"color:#c69005\"> 2. How we evaluate our algorithms </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57713944",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.1 **Basic Metrics** </span>\n",
    "- **Accuracy**: Proportion of correct predictions.\n",
    "- **Precision**: Of the cases predicted as positive, how many were actually positive.\n",
    "- **Recall (Sensitivity)**: Of all actual positive cases, how many were correctly identified.\n",
    "- **F1-Score**: Harmonic mean of precision and recall.\n",
    "- **Specificity**: Of all actual negative cases, how many were correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37155f37",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.2 **Confusion Matrix** </span>\n",
    "- **True Positives (TP)**: Positive cases correctly identified.\n",
    "- **False Positives (FP)**: Negative cases incorrectly classified as positive.\n",
    "- **True Negatives (TN)**: Negative cases correctly identified.\n",
    "- **False Negatives (FN)**: Positive cases incorrectly classified as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f938425",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.3 **Advanced Metrics** </span>\n",
    "- **ROC Curve**: Plots the true positive rate vs. false positive rate.\n",
    "- **Area Under the Curve (AUC)**: Measures classifier performance across different thresholds.\n",
    "- **Log Loss**: Evaluates the quality of predicted probabilities.\n",
    "- **Cohenâ€™s Kappa Coefficient**: Measures agreement between predicted and actual labels, accounting for chance agreement."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color:#f6f794\"> **Supervised ML: Classification Algorithms** </span> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#c69005\"> What is a \"**Classification Algorithms**\" ? </span>\n",
    "\n",
    "Classification is a type of supervised learning where the goal is to <span style=\"color:#F2C122\"> **assign an input** </span> to one of several predefined categories or classes based on its features. Each input is labeled with a class, and the model learns to predict the class for new, unseen data.\n",
    "\n",
    "* <i> **Classification:** The computer classifies new pictures into categories. </i>\n",
    "* <i> Example: Given an image of a flower, classifying it as “rose,” “tulip,” or “daisy.” </i>\n",
    "\n",
    "#### Differences with \"similar\" techniques: \n",
    "\n",
    "| **Techniques**        | **Description**                                               | **Key Difference with Classification**                                        |\n",
    "|-----------------------|---------------------------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| **Classification**     | <span style=\"color:#F2C122\"> **Assigning** </span> an input into one of several predefined categories or classes. | <span style=\"color:#F2C122\"> **Each input is assigned to one class** </span> .                                          |\n",
    "| **Segmentation**       | <span style=\"color:#F2C122\"> **Dividing** </span> data into meaningful segments or regions (often in images or text). | Involves dividing data into parts, but not necessarily assigning each part to a class. |\n",
    "| **Clustering**         | <span style=\"color:#F2C122\"> **Grouping** </span> similar data points together without predefined labels. | Involves grouping data but without the aim of classifying each individual item. |\n",
    "| **Regression**         | <span style=\"color:#F2C122\"> **Predicting** </span> a continuous value (numeric) based on input data. | Classification involves discrete classes, while regression <u> **deals with continuous outputs** </u> . |\n",
    "| **Anomaly Detection**  | <span style=\"color:#F2C122\"> **Identifying** </span> unusual or outlier data points that deviate from normal patterns. | Classification assigns labels, while anomaly detection <u> **identifies data points that don’t fit the pattern** </u> . |\n",
    "\n",
    "#### <span style=\"color:#c69005\"> **Types** of Supervised Classification Algorithms: </span>\n",
    "\n",
    "* **Logistic Regression:**\n",
    "* **Decision Trees:**\n",
    "* **Random Forests:**\n",
    "* **Support Vector Machines (SVMs):**\n",
    "* **K-Nearest Neighbors (KNN):**\n",
    "* **Naive Bayes:**\n",
    "* **Neural Networks (Deep Learning):**\n",
    " \n",
    "\n",
    "#### <span style=\"color:#c69005\"> **What** Do These Algorithms Do? </span>\n",
    "\n",
    "They learn patterns from labeled data and predict classes for new data.\n",
    "\n",
    "#### <span style=\"color:#c69005\"> **How** Are They Useful: </span>\n",
    "\n",
    "Classification algorithms are useful when you have **labeled** data, and your goal is to **categorize** new data points into one of several predefined **classes** or labels.\n",
    "\n",
    "#### <span style=\"color:#c69005\"> **Why** are Classification Algorithms Useful?  </span>\n",
    "\n",
    "* **Automation of Decision-Making:** Classification helps automate processes like fraud detection, recommendation systems, or medical diagnosis.\n",
    "* **Improved Accuracy:** It helps in making predictions about unseen data based on learned patterns from the training data.\n",
    "* **Efficient Categorization:** Classification helps categorize vast amounts of data quickly and accurately, enabling businesses to make data-driven decisions.\n",
    "* **Risk Management:** Helps in identifying risks, such as detecting fraudulent transactions or predicting equipment failure.\n",
    "\n",
    "#### <span style=\"color:#c69005\"> **When** to Use Classification Algorithms </span>\n",
    "\n",
    "- When you:\n",
    "    - Have labeled data.\n",
    "    - Want to predict categories.\n",
    "    - Need to automate classification.\n",
    "    - Have a clear target variable.\n",
    "\n",
    "\n",
    "#### <span style=\"color:#c69005\"> Key **Advantages** of Classification Algorithms: </span>\n",
    "\n",
    "* **Versatility:** Can be applied to a wide range of domains (e.g., finance, healthcare, marketing).\n",
    "* **Scalability:** Can handle large datasets, especially with algorithms.\n",
    "* **Interpretability:** Many classification algorithms provide insight into how decisions are made, which is useful for understanding model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# <center> <span style=\"color:#f6f794\">**Types** of supervised classification algorithms: </center> </span>\n",
    "\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Logistic Regression**  </span>\n",
    "\n",
    "* What is it?\n",
    "    * A linear probabilistic classifier that predicts the probability of a binary class. It uses a sigmoid function to transform the output into probabilities.\n",
    "* What it does?\n",
    "    * It generates a linear decision boundary that separates the classes.\n",
    "* How it works?\n",
    "    * The algorithm finds the coefficients (weights) that minimize the log loss function, adjusting the probability of belonging to each class. It uses the sigmoid function and this converts the output to a probability between 0 and 1.\n",
    "* When to choose it? \n",
    "    - Linearly separable data.\n",
    "    - No multicollinearity among variables.\n",
    "    - Binary classification (2 classes).\n",
    "* Example:\n",
    "    * Predicting if an email is spam based on features like message length and keyword frequency. \n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It draws a straight line to separate two groups and tells you how likely something belongs to one of them.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Decision Trees**  </span>\n",
    "\n",
    "* What is it?\n",
    "    * A non-linear classifier that splits data using simple rules into subsets at each decision node.\n",
    "* What it does?\n",
    "    * It divides the data according to features until each terminal node (leaf) contains only one class.\n",
    "* How it works?\n",
    "    * The tree is built using a greedy algorithm that maximizes information gain (based on measures like entropy or Gini impurity). Each node asks a question about a feature, and based on the answer, it splits the data.\n",
    "* When to choose it? \n",
    "    - Data with complex interaction.\n",
    "    - Categorical or numerical variables.\n",
    "    - Need interpretability (understanding how the decision was made).\n",
    "    - suitable when data contains clear decision rules that lead to different outcomes\n",
    "* Example:\n",
    "    * Classifying whether a person will buy a product based on age, income, and location.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It asks yes/no questions about your data until it decides what class it belongs to—like playing 20 questions.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Random Forest**  </span>\n",
    "* What is it?\n",
    "    * An ensemble of many decision trees, where each tree is trained on a random subset of data and features.\n",
    "* What it does?\n",
    "    * Each tree makes a prediction, and the final result is determined by the majority vote across all trees.\n",
    "* How it works?\n",
    "    * Trains multiple trees on bootstrap samples of the data, and for each split, it selects a random subset of features. The predictions are aggregated through voting (for classification).\n",
    "* When to choose it? \n",
    "    - Large and complex relations in large datasets.\n",
    "    - You want a robust model that generalizes well.\n",
    "    - Avoid overfitting. \n",
    "    - High-dimensional data\n",
    "* Example:\n",
    "    * Classifying images of animals, where each tree predicts the animal in the image, and the majority vote decides the type.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It grows a bunch of decision trees and lets them vote. The majority decides the final class.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Support Vector Machines (SVM)**  </span>\n",
    "* What is it?\n",
    "    * A classifier that finds the hyperplane that maximizes the margin between classes. It can use kernels to handle non-linear data.\n",
    "* What it does?\n",
    "    * It finds the optimal hyperplane that separates the classes, and if data is non-linear, it uses a kernel to transform the space into one where the classes are linearly separable.\n",
    "* How it works?\n",
    "    * The algorithm solves an optimization problem to maximize the margin between classes. It uses Lagrange multipliers to find the optimal separating hyperplane. Kernels like RBF (Radial basis function) map data into higher dimensions to make it linearly separable.\n",
    "* When to choose it? \n",
    "    - Well-separated data (clear class separation).\n",
    "    - High-dimensional feature space.\n",
    "    - Small to medium-sized datasets.\n",
    "* Example:\n",
    "    * Classifying images of cats and dogs, where SVM separates the classes using a large margin between the data points.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It finds the best line (or curve) that separates your classes with the biggest possible space in between.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#c69005\"> **K-Nearest Neighbors (KNN)**  </span>\n",
    "* What is it?\n",
    "    * A lazy learner that classifies a data point based on the k nearest neighbors.\n",
    "* What it does?\n",
    "    * It assigns a class to a new data point by finding the k nearest points and selecting the most common class among them.\n",
    "* How it works?\n",
    "    * For each new data point, it calculates the Euclidean distance (or another distance metric) between the point and the training data, and assigns the class of the k (k: number of clusters/groups) closest points.\n",
    "* When to choose it? \n",
    "    - Clear distribution of data (clear separation between classes based on proximity).\n",
    "    - Small datasets.\n",
    "    - Simplicity and speed (relatively simple data).\n",
    "* Example:\n",
    "    * Predicting if a person has diabetes based on the proximity to other people with similar characteristics.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It looks at the closest neighbors and classifies based on what most of them are.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Naive Bayes**  </span>\n",
    "* What is it?\n",
    "    * A probabilistic classifier that uses Bayes' Theorem under the assumption of independent features.\n",
    "* What it does?\n",
    "    * It calculates the posterior probability of each class and assigns the class with the highest probability.\n",
    "* How it works?\n",
    "    * It applies **Bayes' Theorem** to calculate the posterior probability. Each feature’s contribution is assumed to be independent given the class.\n",
    "* When to choose it? \n",
    "    - Independent features.\n",
    "    - Text classification tasks, like spam detection or sentiment analysis.\n",
    "* Example:\n",
    "    * Classifying emails as \"spam\" or \"not spam\" based on keyword frequencies.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It uses probabilities to guess the class, assuming each feature acts on its own.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Neural Networks (Deep Learning)**  </span>\n",
    "* What is it?\n",
    "    * A model inspired by the human brain, consisting of layers of artificial neurons that learn complex representations of the data.\n",
    "* What it does?\n",
    "    * It adjusts the weights of connections between neurons to minimize the prediction error.\n",
    "* How it works?\n",
    "    * Each neuron applies an activation function (like ReLU) and adjusts the weights through backpropagation and gradient descent to minimize the loss.\n",
    "* When to choose it? \n",
    "    - Large datasets.\n",
    "    - Non-linear complex problems.\n",
    "    - High accuracy is required, and interpretability is less important.\n",
    "    - Unstructured datasets\n",
    "* Example:\n",
    "    * Classifying images of objects into categories like \"cat\", \"dog\", \"car\".\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It’s like a big brain with layers that learns patterns—great for images, sound, and messy data.\n",
    "\n",
    "<br>\n",
    "\n",
    "#\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "\n",
    "| <span style=\"color:#F2C122\"> **Situation**   </span>                         | <span style=\"color:#F2C122\"> **Data Requirements / Technical Notes** </span> | <span style=\"color:#F2C122\"> **Recommended Algorithm** </span> |\n",
    "|------------------------------------------|--------------------------------------------------------------------------|-----------------------------|\n",
    "| **Linearly separable and binary data**   | Requires linearity between features and the log-odds; sensitive to multicollinearity and outliers; features should be scaled. | **Logistic Regression**     |\n",
    "| **Complex relationships among features** | No need for feature scaling; robust to outliers; can handle both numerical and categorical data; prone to overfitting. | **Decision Trees**          |\n",
    "| **Large datasets and robustness needed** | Handles missing values and outliers well; does not assume normality or homoscedasticity; no need for feature scaling. | **Random Forest**           |\n",
    "| **Small sample size, high-dimensional**  | Requires feature scaling; sensitive to outliers; works best when classes are well-separated (margin-based). | **SVM**                     |\n",
    "| **Clear data distribution and speed**    | Requires feature scaling; sensitive to noise and irrelevant features; assumes local homogeneity. | **KNN**                     |\n",
    "| **Text or discrete variables**           | Assumes feature independence; works well even with small datasets; robust to irrelevant features but sensitive to zero probabilities. | **Naive Bayes**             |\n",
    "| **Non-linear, high complexity data**     | Requires large datasets; sensitive to feature scaling; can overfit if not regularized; data should be preprocessed (normalized, encoded). | **Neural Networks**         |\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# <center> <span style=\"color:#f6f794\"> Key words: </span> </center>\n",
    "\n",
    "- **Supervised Learning**: A type of machine learning where the model is trained with labeled data to make predictions.\n",
    "- **Model**: A mathematical representation or algorithm that learns from data to make predictions or decisions.\n",
    "- **Labeled Data**: Data where each input is paired with a known output (label).\n",
    "- **Mapping**: The process of learning a relationship between input data and output labels in supervised learning.\n",
    "- **Input Features**: The data used by the model to make predictions (e.g., words, numbers, images).\n",
    "- **Output Labels**: The correct answers or categories assigned to the input features during training.\n",
    "- **Training**: The process of teaching a model by providing it with labeled data so it can learn.\n",
    "- **Loss Function**: A function that measures how far the model's predictions are from the actual answers, which the model tries to minimize.\n",
    "- **Prediction**: The result or output produced by the model based on new input data.\n",
    "- **Classification**: A type of supervised learning where the model assigns input data into categories or classes.\n",
    "- **Regression**: A type of supervised learning where the model predicts continuous numerical values.\n",
    "- **Generalization**: The model's ability to make accurate predictions on new, unseen data.\n",
    "- **Optimization**: The process of adjusting the model's parameters to minimize errors and improve performance.\n",
    "- **Gradient Descent**: A popular optimization technique used to adjust model parameters by reducing the loss function.\n",
    "- **Linear Probabilistic**: A method (like logistic regression) where the prediction is a probability derived from a linear combination of inputs.\n",
    "- **Linear Decision Boundary**: A straight line or hyperplane that separates different classes in the feature space.\n",
    "- **Log Loss Function**: Also called binary cross-entropy, it measures the performance of a classification model by penalizing incorrect predictions more strongly when the model is confident but wrong.\n",
    "- **Entropy**: A measure of disorder or uncertainty. In decision trees, it measures how mixed the class labels are in a dataset.\n",
    "- **Gini Impurity**: Another measure used in decision trees to evaluate the quality of a split; it represents the probability of incorrectly classifying a randomly chosen element.\n",
    "- **Hyperplane**: A decision boundary in SVMs that separates classes in a high-dimensional space; in 2D, it’s a line, in 3D, a plane, and beyond that, a hyperplane.\n",
    "- **Kernel**: A function that transforms data into a higher-dimensional space to make it easier to find a separating hyperplane (used in SVMs).\n",
    "- **Lagrange Multipliers**: A mathematical technique used in SVM optimization to maximize the margin while satisfying constraints.\n",
    "- **Euclidean Distance**: A way of measuring straight-line distance between two points in space, often used in KNN to find nearest neighbors.\n",
    "\n",
    "# <center>  <span style=\"color:#f6f794\"> Key words but lets make it easier: </span> </center>\n",
    "\n",
    "- **Supervised Learning**: Teaching a computer to make decisions by showing it examples with the right answers.\n",
    "- **Model**: A tool (like a robot brain) that learns from data to help make decisions or predictions.\n",
    "- **Labeled Data**: Information where we already know the correct answer or category (like marking emails as \"spam\" or \"not spam\").\n",
    "- **Mapping**: The process of connecting what the computer learns from the data to the right answer.\n",
    "- **Input Features**: The information the computer uses to make its guess (like pictures, words, or numbers).\n",
    "- **Output Labels**: The correct answer or category that we want the computer to predict (like \"spam\" or \"not spam\").\n",
    "- **Training**: The process where the computer looks at examples and learns to make predictions.\n",
    "- **Loss Function**: A way to measure how far off the computer’s guesses are from the right answer, and the goal is to make it as small as possible.\n",
    "- **Prediction**: The computer's best guess about what the answer should be for new information it has never seen before.\n",
    "- **Classification**: A type of learning where the computer puts things into different groups or categories.\n",
    "- **Regression**: A type of learning where the computer predicts a number, like guessing the price of a house.\n",
    "- **Generalization**: When the computer is good at making correct predictions, even for new things it hasn’t seen before.\n",
    "- **Optimization**: Adjusting the computer's thinking to improve its decision-making and get better at making predictions.\n",
    "- **Gradient Descent**: A method the computer uses to get better by learning from its mistakes and slowly improving its guesses.\n",
    "- **Linear Probabilistic**: The computer uses math to guess a probability, like “there’s a 90% chance this is spam,” based on a straight-line relationship.\n",
    "- **Linear Decision Boundary**: A line (or flat surface) that the computer draws to separate things into groups—like drawing a line between apples and oranges.\n",
    "- **Log Loss Function**: A way of scoring how wrong a prediction is, especially when the computer is very sure but ends up being wrong—it’s a harsh teacher!\n",
    "- **Entropy**: A way to measure uncertainty or “messiness” in the data—the more mixed up the groups are, the higher the entropy.\n",
    "- **Gini Impurity**: Another way to measure how mixed a group is—used to help decision trees figure out how to split the data.\n",
    "- **Hyperplane**: A fancy word for a line or surface the computer uses to separate groups, especially in high dimensions (like 3D or more).\n",
    "- **Kernel**: A trick that helps the computer look at data in a new way so that it can find patterns even when things are not clearly separated.\n",
    "- **Lagrange Multipliers**: A math tool used behind the scenes to help find the best way to separate groups while following certain rules.\n",
    "- **Euclidean Distance**: Just the straight-line distance between two points—like using a ruler to measure how close two things are.\n",
    "\n",
    "<br>\n",
    "\n",
    "#\n",
    "\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "______\n",
    "\n",
    "# <center>  <span style=\"color:#f6f794\"> Useful links: </span> </center> \n",
    "\n",
    "https://datascientest.com/en/classification-algorithms-definition-and-main-models#:~:text=Classification%20algorithms%20are%20part%20of,and%20then%20predictions%20are%20made.\n",
    "\n",
    "https://www.ibm.com/think/topics/classification-machine-learning\n",
    "\n",
    "https://www.sciencedirect.com/topics/engineering/classification-algorithm\n",
    "\n",
    "https://www.simplilearn.com/tutorials/machine-learning-tutorial/classification-in-machine-learning#what_is_classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color:#f6f794\"> **Supervised ML: Classification Algorithms** </span> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#c69005\"> What is a \"**Classification Algorithms**\" ? </span>\n",
    "\n",
    "Classification is a type of supervised learning where the goal is to <span style=\"color:#F2C122\"> **assign an input** </span> to one of several predefined categories or classes based on its features. Each input is labeled with a class, and the model learns to predict the class for new, unseen data.\n",
    "\n",
    "* <i> **Classification:** The computer classifies new pictures into categories. </i>\n",
    "* <i> Example: Given an image of a flower, classifying it as “rose,” “tulip,” or “daisy.” </i>\n",
    "\n",
    "#### Differences with \"similar\" techniques: \n",
    "\n",
    "| **Techniques**        | **Description**                                               | **Key Difference with Classification**                                        |\n",
    "|-----------------------|---------------------------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| **Classification**     | <span style=\"color:#F2C122\"> **Assigning** </span> an input into one of several predefined categories or classes. | <span style=\"color:#F2C122\"> **Each input is assigned to one class** </span> .                                          |\n",
    "| **Segmentation**       | <span style=\"color:#F2C122\"> **Dividing** </span> data into meaningful segments or regions (often in images or text). | Involves dividing data into parts, but not necessarily assigning each part to a class. |\n",
    "| **Clustering**         | <span style=\"color:#F2C122\"> **Grouping** </span> similar data points together without predefined labels. | Involves grouping data but without the aim of classifying each individual item. |\n",
    "| **Regression**         | <span style=\"color:#F2C122\"> **Predicting** </span> a continuous value (numeric) based on input data. | Classification involves discrete classes, while regression <u> **deals with continuous outputs** </u> . |\n",
    "| **Anomaly Detection**  | <span style=\"color:#F2C122\"> **Identifying** </span> unusual or outlier data points that deviate from normal patterns. | Classification assigns labels, while anomaly detection <u> **identifies data points that don’t fit the pattern** </u> . |\n",
    "\n",
    "#### <span style=\"color:#c69005\"> **Types** of Supervised Classification Algorithms: </span>\n",
    "\n",
    "* **Logistic Regression:**\n",
    "* **Decision Trees:**\n",
    "* **Random Forests:**\n",
    "* **Support Vector Machines (SVMs):**\n",
    "* **K-Nearest Neighbors (KNN):**\n",
    "* **Naive Bayes:**\n",
    "* **Neural Networks (Deep Learning):**\n",
    " \n",
    "\n",
    "#### <span style=\"color:#c69005\"> **What** Do These Algorithms Do? </span>\n",
    "\n",
    "They learn patterns from labeled data and predict classes for new data.\n",
    "\n",
    "#### <span style=\"color:#c69005\"> **How** Are They Useful: </span>\n",
    "\n",
    "Classification algorithms are useful when you have **labeled** data, and your goal is to **categorize** new data points into one of several predefined **classes** or labels.\n",
    "\n",
    "#### <span style=\"color:#c69005\"> **Why** are Classification Algorithms Useful?  </span>\n",
    "\n",
    "* **Automation of Decision-Making:** Classification helps automate processes like fraud detection, recommendation systems, or medical diagnosis.\n",
    "* **Improved Accuracy:** It helps in making predictions about unseen data based on learned patterns from the training data.\n",
    "* **Efficient Categorization:** Classification helps categorize vast amounts of data quickly and accurately, enabling businesses to make data-driven decisions.\n",
    "* **Risk Management:** Helps in identifying risks, such as detecting fraudulent transactions or predicting equipment failure.\n",
    "\n",
    "#### <span style=\"color:#c69005\"> **When** to Use Classification Algorithms </span>\n",
    "\n",
    "- When you:\n",
    "    - Have labeled data.\n",
    "    - Want to predict categories.\n",
    "    - Need to automate classification.\n",
    "    - Have a clear target variable.\n",
    "\n",
    "\n",
    "#### <span style=\"color:#c69005\"> Key **Advantages** of Classification Algorithms: </span>\n",
    "\n",
    "* **Versatility:** Can be applied to a wide range of domains (e.g., finance, healthcare, marketing).\n",
    "* **Scalability:** Can handle large datasets, especially with algorithms.\n",
    "* **Interpretability:** Many classification algorithms provide insight into how decisions are made, which is useful for understanding model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# <center> <span style=\"color:#f6f794\">**Types** of supervised classification algorithms: </center> </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../img/Logistic Regression.png\" width=\"900\" height=\"300\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> **Logistic Regression**  </span>\n",
    "\n",
    "* What is it?\n",
    "    * A linear probabilistic classifier that predicts the probability of a binary class. It uses a sigmoid function to transform the output into probabilities.\n",
    "* What it does?\n",
    "    * It generates a linear decision boundary that separates the classes.\n",
    "* How it works?\n",
    "    * The algorithm finds the coefficients (weights) that minimize the log loss function, adjusting the probability of belonging to each class. It uses the sigmoid function and this converts the output to a probability between 0 and 1.\n",
    "* When to choose it? \n",
    "    - Linearly separable data.\n",
    "    - No multicollinearity among variables.\n",
    "    - Binary classification (2 classes).\n",
    "* Example:\n",
    "    * Predicting if an email is spam based on features like message length and keyword frequency. \n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    - Calculates a score based on all the features\n",
    "    - Converts that score into a probability (between 0 and 1)\n",
    "    - Decides the category based on a threshold (usually 0.5)\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "| Feature        | Advantages                                                                  | Limitations                                                              |\n",
    "| :--------------- | :------------------------------------------------------------------------ | :------------------------------------------------------------------------ |\n",
    "| **Output** | Gives us probabilities, not just yes/no answers                           |                                                                           |\n",
    "| **Simplicity** | Relatively simple to understand                                           |                                                                           |\n",
    "| **Interpretation** | We can interpret how each feature influences the prediction            | Does not handle complex relationships between variables                     |\n",
    "| **Data** | Works well with many data points                                          | Can have problems when there are many more examples of one class than another |\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../img/KNN (K Nearest Neighbor).png\" width=\"900\" height=\"300\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> **K-Nearest Neighbors (KNN)**  </span>\n",
    "* What is it?\n",
    "    * A lazy learner that classifies a data point based on the k nearest neighbors.\n",
    "* What it does?\n",
    "    * It assigns a class to a new data point by finding the k nearest points and selecting the most common class among them.\n",
    "* How it works?\n",
    "    * For each new data point, it calculates the Euclidean distance (or another distance metric) between the point and the training data, and assigns the class of the k (k: number of clusters/groups) closest points.\n",
    "* When to choose it? \n",
    "    - Clear distribution of data (clear separation between classes based on proximity).\n",
    "    - Small datasets.\n",
    "    - Simplicity and speed (relatively simple data).\n",
    "* Example:\n",
    "    * Predicting if a person has diabetes based on the proximity to other people with similar characteristics.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It looks at the closest neighbors and classifies based on what most of them are.\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "| Feature          | Advantages                                                                | Limitations                                                                 |\n",
    "| :------------------ | :---------------------------------------------------------------------- | :--------------------------------------------------------------------------- |\n",
    "| **Understanding** | Very intuitive and easy to understand                                     | Slow with large amounts of data                                        |\n",
    "| **Assumptions** | Does not make assumptions about the structure of the data                  | Gets confused by irrelevant features                                 |\n",
    "| **Separation** | Works well when categories are clearly separated                          | Requires choosing the number of \"neighbors\" to consider carefully                             |\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../img/Support Vector Machines (SVM).png\" width=\"900\" height=\"300\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> **Support Vector Machines (SVM)** </span>\n",
    "* What is it?\n",
    "    * A classifier that finds the hyperplane (dividing line) that maximizes the margin between classes. It can use kernels to handle non-linear data.\n",
    "* What does it do?\n",
    "    * Finds the optimal hyperplane that separates the classes, and if the data is non-linear, it uses a kernel to transform the space to one where the classes are linearly separable.\n",
    "* How does it work?\n",
    "    * The SVM searches for the best \"dividing line\" (hyperplane) between groups of data, maximizing the distance (margin) to the closest points. For complex separations, it uses \"special functions\" (kernels) that transform the data.\n",
    "* When to choose it?\n",
    "    - Well-separated data (clear separation between classes).\n",
    "    - High-dimensional feature space.\n",
    "    - Small or medium-sized datasets.\n",
    "* Example:\n",
    "    * Classifying images of cats and dogs, where SVM separates the classes using a large margin between the data points.\n",
    "* <span style=\"color:#F2C122\"> **In simple terms**: </span>\n",
    "    - Finds the boundary that best separates the categories\n",
    "    - Uses only the points closest to the boundary (support vectors)\n",
    "    - For complex problems, uses kernels (\"mathematical functions\") to work in higher-dimensional spaces\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "| Feature            | Advantages                                                                 | Limitations                                                              |\n",
    "| :-------------------- | :----------------------------------------------------------------------- | :------------------------------------------------------------------------ |\n",
    "| **Dimensionality** | Very effective in spaces with many dimensions                           | Does not work well with very large datasets                               |\n",
    "| **Separation** | Robust when there is a clear separation between classes                |                                                                           |\n",
    "| **Flexibility** | Versatile thanks to different kernel functions                           | Parameter selection is critical                                   |\n",
    "| **Output** |                                                                          | Does not directly provide probabilities                                  |\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../img/Decision Tree.png\" width=\"900\" height=\"300\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> **Decision Trees**  </span>\n",
    "\n",
    "* What is it?\n",
    "    * A non-linear classifier that splits data using simple rules into subsets at each decision node.\n",
    "* What it does?\n",
    "    * It divides the data according to features until each terminal node (leaf) contains only one class.\n",
    "* How it works?\n",
    "    * The tree is built using a greedy algorithm that maximizes information gain (based on measures like entropy or Gini impurity). Each node asks a question about a feature, and based on the answer, it splits the data.\n",
    "* When to choose it? \n",
    "    - Data with complex interaction.\n",
    "    - Categorical or numerical variables.\n",
    "    - Need interpretability (understanding how the decision was made).\n",
    "    - suitable when data contains clear decision rules that lead to different outcomes\n",
    "* Example:\n",
    "    * Classifying whether a person will buy a product based on age, income, and location.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It asks yes/no questions about your data until it decides what class it belongs to—like playing 20 questions.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "| Feature          | Advantages                                                              | Limitations                                                                |\n",
    "| :------------------ | :-------------------------------------------------------------------- | :-------------------------------------------------------------------------- |\n",
    "| **Understanding** | Very easy to understand and interpret                               | Tends to memorize the training data (overfitting)                |\n",
    "| **Preparation** | Requires little data preparation                                   | Small changes in the data can generate very different trees        |\n",
    "| **Variables** | Handles different types of variables well                             | Does not always find simple patterns                                       |\n",
    "| **Complexity** | Can capture complex relationships                                 |                                                                             |\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../img/Random Forest.png\" width=\"900\" height=\"300\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> **Random Forest**  </span>\n",
    "* What is it?\n",
    "    * An ensemble of many decision trees, where each tree is trained on a random subset of data and features.\n",
    "* What it does?\n",
    "    * Each tree makes a prediction, and the final result is determined by the majority vote across all trees.\n",
    "* How it works?\n",
    "    * Trains multiple trees on bootstrap samples of the data, and for each split, it selects a random subset of features. The predictions are aggregated through voting (for classification).\n",
    "* When to choose it? \n",
    "    - Large and complex relations in large datasets.\n",
    "    - You want a robust model that generalizes well.\n",
    "    - Avoid overfitting. \n",
    "    - High-dimensional data\n",
    "* Example:\n",
    "    * Classifying images of animals, where each tree predicts the animal in the image, and the majority vote decides the type.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It grows a bunch of decision trees and lets them vote. The majority decides the final class.\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "| Feature         | Advantages                                                               | Limitations                                                                |\n",
    "| :--------------------- | :--------------------------------------------------------------------- | :-------------------------------------------------------------------------- |\n",
    "| **Accuracy** | More accurate than a single tree                                        | Less interpretable than a single tree                                     |\n",
    "| **Overfitting** | Significantly reduces overfitting                                    | Requires more computational resources                                     |\n",
    "| **Robustness** | Robust against outliers and noise                                    | Can be slow for real-time predictions                                    |\n",
    "| **Importance** | Provides measures of feature importance                                |                                                                             |\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../img/Gradient Boosting.png\" width=\"900\" height=\"300\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> **Gradient Boosting** </span>\n",
    "* What is it?\n",
    "    * Iteratively builds a predictive model by combining multiple weak models (usually decision trees) to create a strong model.\n",
    "* What does it do?\n",
    "    * Predicts the target variable by learning from the errors of previous models. Each new model focuses on correcting the mistakes made by the combination of the previous models.\n",
    "* How does it work?\n",
    "    * Starts with a base model (often a shallow decision tree). Then, it sequentially trains new trees to predict the residuals (the difference between the current prediction and the actual value) of the previous model. The predictions of all the trees are summed to obtain the final prediction.\n",
    "* When to choose it?\n",
    "    - When high accuracy and performance are sought.\n",
    "    - For classification and regression problems.\n",
    "    - On medium to large datasets.\n",
    "    - When hyperparameters can be tuned to optimize performance.\n",
    "* Example:\n",
    "    * Predicting the credit risk of a customer, where each successive tree learns from the prediction errors of the previous trees, improving overall accuracy.\n",
    "* <span style=\"color:#F2C122\"> **In simple terms**: </span>\n",
    "    * Builds the model step by step, where each new step tries to correct the errors of the previous step (learns from the previous model to improve progressively).\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "| Important Variants            | Advantages                                          | Limitations                                              |\n",
    "| :-------------------------------- | :------------------------------------------------ | :-------------------------------------------------------- |\n",
    "| **XGBoost**: Optimized version | Generally more accurate than Random Forest       | Can over-memorize the training data                     |\n",
    "| **LightGBM**: Focused on speed | Handles different types of data well          | More sensitive to outliers                               |\n",
    "| **CatBoost**: Handles categoricals well | Identifies important features                 | More complicated to configure correctly                  |\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../img/Neural Networks.png\" width=\"900\" height=\"300\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> **Neural Networks (Deep Learning)**  </span>\n",
    "* What is it?\n",
    "    * A model inspired by the human brain, consisting of layers of artificial neurons that learn complex representations of the data.\n",
    "* What it does?\n",
    "    * It adjusts the weights of connections between neurons to minimize the prediction error.\n",
    "* How it works?\n",
    "    * Each neuron applies an activation function (like ReLU) and adjusts weights through backpropagation and gradient descent to minimize the loss.\n",
    "        - Organized in layers of interconnected \"neurons\"\n",
    "        - Each neuron processes information and passes it to the next layer\n",
    "        - The network learns by gradually adjusting the connections\n",
    "* When to choose it? \n",
    "    - Large datasets.\n",
    "    - Non-linear complex problems.\n",
    "    - High accuracy is required, and interpretability is less important.\n",
    "    - Unstructured datasets\n",
    "* Example:\n",
    "    * Classifying images of objects into categories like \"cat\", \"dog\", \"car\".\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * It's like a big brain with layers that learns complex patterns in the data\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "| Advantages                                                  | Limitations                                                 |\n",
    "| :-------------------------------------------------------- | :----------------------------------------------------------- |\n",
    "| Can capture extremely complex relationships              | Requires a lot of data to work well                         |\n",
    "| Excellent for large amounts of data                      | Difficult to interpret (black box)                            |\n",
    "| Versatile for different types of information (images, text, audio) | Computationally intensive                                  |\n",
    "|                                                           | Many parameters to adjust                                     |\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"../img/Naive Bayes.png\" width=\"900\" height=\"300\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> **Naive Bayes**  </span>\n",
    "* What is it?\n",
    "    * A probabilistic classifier that uses Bayes' Theorem under the assumption of independent features.\n",
    "* What it does?\n",
    "    * It calculates the posterior probability of each class and assigns the class with the highest probability.\n",
    "* How it works?\n",
    "    * Applies **Bayes' Theorem** to calculate the posterior probability. It is assumed that the contribution of each feature is independent given the class.\n",
    "        - Calculates the probability of each category\n",
    "        - Calculates the probability of each feature given a category\n",
    "        - Combines these probabilities to determine the most likely category\n",
    "* When to choose it? \n",
    "    - Independent features.\n",
    "    - Text classification tasks, like spam detection or sentiment analysis.\n",
    "* Example:\n",
    "    * Classifying emails as \"spam\" or \"not spam\" based on keyword frequencies.\n",
    "* <span style=\"color:#F2C122\">  **In simple words**: </span> \n",
    "    * Uses probabilities to predict categories, assuming features are independent of each other (although in real life they rarely are).\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "| Advantages                                     | Limitations                                               |\n",
    "| :------------------------------------------- | :--------------------------------------------------------- |\n",
    "| Fast and simple to train                      | The assumption of independence is rarely true             |\n",
    "| Works well even with small datasets            | Can be outperformed by more sophisticated models         |\n",
    "| Efficient with many features                   | Does not capture relationships between features             |\n",
    "| Good for text classification                 |                                                            |\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# <center> <span style=\"color:#f6f794\"> How to Evaluate Classification Models? </span> </center>\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Basic Metrics** </span>\n",
    "\n",
    "- **Accuracy**: What percentage of predictions are correct?\n",
    "- **Precision**: Of the cases I predicted as positive, how many were actually positive?\n",
    "- **Recall (Sensitivity)**: Of all the actually positive cases, how many did I correctly identify?\n",
    "- **F1-Score**: Balance between precision and recall\n",
    "- **Confusion Matrix**: Table showing correct and incorrect predictions for each category\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Practical Example** </span>\n",
    "\n",
    "Let's imagine a model that detects diseases:\n",
    "\n",
    "- **False Positives**: Healthy people incorrectly diagnosed as sick\n",
    "- **False Negatives**: Sick people incorrectly diagnosed as healthy\n",
    "\n",
    "In this case, false negatives are much more dangerous, so we would want a model with high recall, even at the cost of having more false positives.\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Data Preparation for Classification** </span>\n",
    "\n",
    "- Handling Missing Data\n",
    "\n",
    "    - **Elimination**: Removing rows or columns with missing values\n",
    "    - **Imputation**: Filling in with values like the mean, median, or predicted values\n",
    "\n",
    "- Normalization\n",
    "\n",
    "    - Adjusting the values so they are on similar scales:\n",
    "\n",
    "        - **Normalization**: Scaling to a specific range (0-1)\n",
    "        - **Standardization**: Transforming to have a mean of 0 and a standard deviation of 1\n",
    "\n",
    "**Example**: If we have age (0-100) and income (thousands or millions), normalizing helps the model not be dominated by the large numbers.\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Encoding Categorical Variables** </span>\n",
    "\n",
    "- Converting text or categories into numbers:\n",
    "\n",
    "    - **One-Hot Encoding**: Creating columns of 0s and 1s for each category\n",
    "    - **Label Encoding**: Assigning a number to each category\n",
    "\n",
    "**Example**: Converting \"red\", \"green\", \"blue\" into [1,0,0], [0,1,0], [0,0,1]\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Model Validation and Improvement** </span>\n",
    "\n",
    "- Cross-Validation\n",
    "\n",
    "    - Instead of splitting the data only once, it's divided into several parts (folds), and the model is trained/evaluated multiple times to get a more reliable measure of performance.\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Hyperparameter Optimization** </span>\n",
    "\n",
    "- Finding the best settings for our model:\n",
    "\n",
    "    - **Grid Search**: Trying all possible combinations of parameters\n",
    "    - **Random Search**: Trying random combinations\n",
    "    - **Bayesian Search**: Using previous results to guide new trials\n",
    "\n",
    "### <span style=\"color:#c69005\"> **Examples of Practical Applications** </span>\n",
    "\n",
    "- **Finance**: Fraud detection, credit approval\n",
    "- **Medicine**: Disease diagnosis, risk prediction\n",
    "- **Marketing**: Customer segmentation, purchase prediction\n",
    "- **Human Resources**: Candidate selection, turnover prediction\n",
    "- **Technology**: Spam filters, image recognition, virtual assistants\n",
    "\n",
    "### <span style=\"color:#c69005\"> **How to Choose the Right Algorithm** </span>\n",
    "\n",
    "1. **Data size**: For small data, Naive Bayes or KNN; for large data, Deep Learning\n",
    "2. **Interpretability**: If you need to explain the model, decision trees or logistic regression\n",
    "3. **Speed**: For fast training, Naive Bayes; for fast prediction, optimized trees\n",
    "4. **Accuracy**: For maximum accuracy, Gradient Boosting or Deep Learning\n",
    "5. **Data type**: For text, Naive Bayes; for images, neural networks\n",
    "\n",
    "The important thing is to test several algorithms for your specific problem and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# <center> <span style=\"color:#f6f794\"> Quick Guide to Choosing a Type: </span> </center>\n",
    "\n",
    "\n",
    "| <span style=\"color:#F2C122\"> **Situation** </span>   | <span style=\"color:#F2C122\"> **Data Requirements** </span> | <span style=\"color:#F2C122\"> **Recommended Algorithm** </span> |\n",
    "|------------------------------------------|--------------------------------------------------------------------------|-----------------------------|\n",
    "| **Linearly separable and binary data** | Requires linearity between features and log-odds; sensitive to multicollinearity and outliers; features should be scaled. | **Logistic Regression** |\n",
    "| **Clear data distribution and speed** | Requires feature scaling; sensitive to noise and irrelevant features; assumes local homogeneity. | **KNN** |\n",
    "| **Small sample size, high dimensionality** | Requires feature scaling; sensitive to outliers; works best when classes are well-separated (based on margins). | **SVM** |\n",
    "| **Complex relationships between features** | Does not require feature scaling; robust to outliers; can handle both numerical and categorical data; prone to overfitting. | **Decision Trees** |\n",
    "| **Large datasets and necessary robustness** | Handles missing values and outliers well; does not assume normality or homoscedasticity; does not need feature scaling. | **Random Forest** |\n",
    "| **Large datasets, improved accuracy over Random Forest** | Similar to Random Forest in data handling, but requires hyperparameter tuning to avoid overfitting; can be sensitive to noise in early stages. | **Gradient Boosting** |\n",
    "| **Non-linear data, high complexity** | Requires large datasets; sensitive to feature scaling; can overfit if not regularized; data should be preprocessed (normalized, encoded). | **Neural Networks** |\n",
    "| **Textual or discrete variables** | Assumes independence between features; works well even with small datasets; robust to irrelevant features but sensitive to zero probabilities. | **Naive Bayes** |\n",
    "\n",
    "<br>\n",
    "\n",
    "#\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# <center> <span style=\"color:#f6f794\"> Key words: </span> </center>\n",
    "\n",
    "- **Supervised Learning**: A type of machine learning where the model is trained with labeled data to make predictions.\n",
    "- **Model**: A mathematical representation or algorithm that learns from data to make predictions or decisions.\n",
    "- **Labeled Data**: Data where each input is paired with a known output (label).\n",
    "- **Mapping**: The process of learning a relationship between input data and output labels in supervised learning.\n",
    "- **Input Features**: The data used by the model to make predictions (e.g., words, numbers, images).\n",
    "- **Output Labels**: The correct answers or categories assigned to the input features during training.\n",
    "- **Training**: The process of teaching a model by providing it with labeled data so it can learn.\n",
    "- **Loss Function**: A function that measures how far the model's predictions are from the actual answers, which the model tries to minimize.\n",
    "- **Prediction**: The result or output produced by the model based on new input data.\n",
    "- **Classification**: A type of supervised learning where the model assigns input data into categories or classes.\n",
    "- **Regression**: A type of supervised learning where the model predicts continuous numerical values.\n",
    "- **Generalization**: The model's ability to make accurate predictions on new, unseen data.\n",
    "- **Optimization**: The process of adjusting the model's parameters to minimize errors and improve performance.\n",
    "- **Gradient Descent**: A popular optimization technique used to adjust model parameters by reducing the loss function.\n",
    "- **Linear Probabilistic**: A method (like logistic regression) where the prediction is a probability derived from a linear combination of inputs.\n",
    "- **Linear Decision Boundary**: A straight line or hyperplane that separates different classes in the feature space.\n",
    "- **Log Loss Function**: Also called binary cross-entropy, it measures the performance of a classification model by penalizing incorrect predictions more strongly when the model is confident but wrong.\n",
    "- **Entropy**: A measure of disorder or uncertainty. In decision trees, it measures how mixed the class labels are in a dataset.\n",
    "- **Gini Impurity**: Another measure used in decision trees to evaluate the quality of a split; it represents the probability of incorrectly classifying a randomly chosen element.\n",
    "- **Hyperplane**: A decision boundary in SVMs that separates classes in a high-dimensional space; in 2D, it’s a line, in 3D, a plane, and beyond that, a hyperplane.\n",
    "- **Kernel**: A function that transforms data into a higher-dimensional space to make it easier to find a separating hyperplane (used in SVMs).\n",
    "- **Lagrange Multipliers**: A mathematical technique used in SVM optimization to maximize the margin while satisfying constraints.\n",
    "- **Euclidean Distance**: A way of measuring straight-line distance between two points in space, often used in KNN to find nearest neighbors.\n",
    "\n",
    "# <center>  <span style=\"color:#f6f794\"> Key words but lets make it easier: </span> </center>\n",
    "\n",
    "- **Supervised Learning**: Teaching a computer to make decisions by showing it examples with the right answers.\n",
    "- **Model**: A tool (like a robot brain) that learns from data to help make decisions or predictions.\n",
    "- **Labeled Data**: Information where we already know the correct answer or category (like marking emails as \"spam\" or \"not spam\").\n",
    "- **Mapping**: The process of connecting what the computer learns from the data to the right answer.\n",
    "- **Input Features**: The information the computer uses to make its guess (like pictures, words, or numbers).\n",
    "- **Output Labels**: The correct answer or category that we want the computer to predict (like \"spam\" or \"not spam\").\n",
    "- **Training**: The process where the computer looks at examples and learns to make predictions.\n",
    "- **Loss Function**: A way to measure how far off the computer’s guesses are from the right answer, and the goal is to make it as small as possible.\n",
    "- **Prediction**: The computer's best guess about what the answer should be for new information it has never seen before.\n",
    "- **Classification**: A type of learning where the computer puts things into different groups or categories.\n",
    "- **Regression**: A type of learning where the computer predicts a number, like guessing the price of a house.\n",
    "- **Generalization**: When the computer is good at making correct predictions, even for new things it hasn’t seen before.\n",
    "- **Optimization**: Adjusting the computer's thinking to improve its decision-making and get better at making predictions.\n",
    "- **Gradient Descent**: A method the computer uses to get better by learning from its mistakes and slowly improving its guesses.\n",
    "- **Linear Probabilistic**: The computer uses math to guess a probability, like “there’s a 90% chance this is spam,” based on a straight-line relationship.\n",
    "- **Linear Decision Boundary**: A line (or flat surface) that the computer draws to separate things into groups—like drawing a line between apples and oranges.\n",
    "- **Log Loss Function**: A way of scoring how wrong a prediction is, especially when the computer is very sure but ends up being wrong—it’s a harsh teacher!\n",
    "- **Entropy**: A way to measure uncertainty or “messiness” in the data—the more mixed up the groups are, the higher the entropy.\n",
    "- **Gini Impurity**: Another way to measure how mixed a group is—used to help decision trees figure out how to split the data.\n",
    "- **Hyperplane**: A fancy word for a line or surface the computer uses to separate groups, especially in high dimensions (like 3D or more).\n",
    "- **Kernel**: A trick that helps the computer look at data in a new way so that it can find patterns even when things are not clearly separated.\n",
    "- **Lagrange Multipliers**: A math tool used behind the scenes to help find the best way to separate groups while following certain rules.\n",
    "- **Euclidean Distance**: Just the straight-line distance between two points—like using a ruler to measure how close two things are.\n",
    "\n",
    "<br>\n",
    "\n",
    "#\n",
    "\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "______\n",
    "\n",
    "# <center>  <span style=\"color:#f6f794\"> Useful links: </span> </center> \n",
    "\n",
    "https://datascientest.com/en/classification-algorithms-definition-and-main-models#:~:text=Classification%20algorithms%20are%20part%20of,and%20then%20predictions%20are%20made.\n",
    "\n",
    "https://www.ibm.com/think/topics/classification-machine-learning\n",
    "\n",
    "https://www.sciencedirect.com/topics/engineering/classification-algorithm\n",
    "\n",
    "https://www.simplilearn.com/tutorials/machine-learning-tutorial/classification-in-machine-learning#what_is_classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

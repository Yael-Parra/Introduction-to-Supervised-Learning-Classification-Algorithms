{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d2ac5a",
   "metadata": {},
   "source": [
    "# <span style=\"color:#f6f794\"> Algoritmos de clasificaci贸n </span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4679eb",
   "metadata": {},
   "source": [
    "## <span style=\"color:#c69005\"> 1. C贸mo funcionan y en qu茅 se diferencian </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1814458",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.1 **Regresi贸n log铆stica** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "A pesar de su nombre, la regresi贸n log铆stica es un algoritmo de clasificaci贸n que estima la probabilidad de que una instancia pertenezca a una clase particular.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**C贸mo funciona**</span>\n",
    "1. Calcula una combinaci贸n lineal de las caracter铆sticas (similar a regresi贸n lineal)\n",
    "2. Aplica la funci贸n log铆stica (sigmoide) para transformar el resultado en una probabilidad entre 0 y 1\n",
    "3. Asigna la clase seg煤n un umbral (t铆picamente 0.5)\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matem谩ticos**</span>\n",
    "  - **Funci贸n log铆sitica**:\n",
    "  $$\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\text{donde } z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$$\n",
    "  - **Probabilidad**:  \n",
    "    $$p(y=1 \\mid \\mathbf{x}) = \\sigma(z)$$\n",
    "  - **Funci贸n de costo** (log-loss o entrop铆a cruzada binaria):  \n",
    "    $$J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]$$\n",
    "  - **Optimizaci贸n**:  \n",
    "    T铆picamente mediante **descenso de gradiente**:\n",
    "    $$\\beta_j := \\beta_j - \\alpha \\frac{\\partial J(\\beta)}{\\partial \\beta_j}$$\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Proporciona probabilidades, no solo predicciones de clase\n",
    "- Relativamente simple y eficiente\n",
    "- Los coeficientes son interpretables como log-odds\n",
    "- Funciona bien con grandes conjuntos de datos\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Asume relaci贸n lineal entre variables y el logaritmo de odds\n",
    "- No maneja bien relaciones no lineales sin transformaci贸n de caracter铆sticas\n",
    "- Puede tener problemas con clases desbalanceadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a92fa",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.2 **K Nearest Neighbors** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "KNN es un algoritmo simple pero efectivo que clasifica una nueva instancia seg煤n la clase mayoritaria de sus k vecinos m谩s cercanos en el espacio de caracter铆sticas.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**C贸mo funciona**</span>\n",
    "1. Almacena todos los datos de entrenamiento\n",
    "2. Para una nueva instancia:\n",
    "   - Calcula la distancia a todas las instancias de entrenamiento\n",
    "   - Identifica los k vecinos m谩s cercanos\n",
    "   - Asigna la clase por voto mayoritario entre estos vecinos\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matem谩ticos**</span>\n",
    "- T铆picamente utiliza **distancia euclidiana**:  \n",
    "    $$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum (x_i - y_i)^2}$$\n",
    "- Otras m茅tricas incluyen distancia de Manhattan, Minkowski o similitud del coseno.\n",
    "- La predicci贸n es:  \n",
    "    $$\\hat{y} = \\arg\\max \\left( \\sum \\mathbb{I}(y_i = c) \\right)$$  \n",
    "    donde \\( \\mathbb{I} \\) es una funci贸n indicadora.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Simple y f谩cil de entender\n",
    "- No hace suposiciones sobre la distribuci贸n de los datos\n",
    "- Funciona bien con clases bien separadas\n",
    "- Naturalmente maneja clasificaci贸n multiclase\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Computacionalmente intensivo para grandes conjuntos de datos\n",
    "- Sensible a caracter铆sticas irrelevantes y a la escala\n",
    "- Requiere selecci贸n cuidadosa del par谩metro k\n",
    "- No produce un modelo expl铆cito (aprendizaje \"perezoso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352eca28",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.3 **SVM (M谩quinas de vectores de soporte)s** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "SVM busca encontrar el hiperplano 贸ptimo que maximiza el margen entre clases en el espacio de caracter铆sticas, potencialmente transformado por una funci贸n kernel.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**C贸mo funciona**</span>\n",
    "1. Encuentra el hiperplano que maximiza la distancia entre las clases\n",
    "2. Los puntos m谩s cercanos al hiperplano se llaman \"vectores de soporte\"\n",
    "3. Para problemas no linealmente separables:\n",
    "   - Utiliza una funci贸n kernel para mapear los datos a un espacio de mayor dimensi贸n\n",
    "   - Encuentra un hiperplano separador en ese espacio\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matem谩ticos**</span>\n",
    "- Busca maximizar el margen:  \n",
    "  $$\\frac{2}{\\|\\mathbf{w}\\|}$$\n",
    "- Con restricci贸n:  \n",
    "  $$y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\text{para todos los puntos}$$\n",
    "- Par谩metro \\( C \\) controla el equilibrio entre maximizar el margen y minimizar el error\n",
    "- Kernels comunes: lineal, polin贸mico, RBF (base radial), sigmoide\n",
    "\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Efectivo en espacios de alta dimensionalidad\n",
    "- Robusto cuando hay una clara separaci贸n entre clases\n",
    "- Usa solo un subconjunto de puntos (vectores de soporte)\n",
    "- Vers谩til a trav茅s de diferentes funciones kernel\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Selecci贸n de par谩metros y kernel cr铆tica\n",
    "- No escalable a grandes conjuntos de datos\n",
    "- No proporciona directamente probabilidades\n",
    "- Sensible a ruido y sobreajuste con ciertos kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae864c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.4 **rboles de decisi贸n para clasificaci贸n** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Los 谩rboles de decisi贸n para clasificaci贸n crean un modelo con estructura de 谩rbol donde cada nodo interno representa una prueba sobre una caracter铆stica, cada rama representa el resultado de esa prueba, y cada hoja representa una clase.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**C贸mo funciona**</span>\n",
    "1. Comienza con todos los datos en el nodo ra铆z\n",
    "2. Para cada divisi贸n potencial:\n",
    "   - Calcula una medida de impureza (Gini o entrop铆a)\n",
    "   - Selecciona la divisi贸n que maximiza la reducci贸n en impureza\n",
    "3. Repite recursivamente en los nodos hijos hasta alcanzar un criterio de parada\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matem谩ticos**</span>\n",
    "- **ndice Gini**: Mide la probabilidad de clasificar incorrectamente un elemento  \n",
    "  $$\\text{Gini} = 1 - \\sum p_i^2$$  \n",
    "  donde \\( p_i \\) es la proporci贸n de la clase \\( i \\)\n",
    "- **Entrop铆a**: Mide la incertidumbre o desorden en los datos  \n",
    "  $$\\text{Entrop铆a} = -\\sum p_i \\log_2(p_i)$$\n",
    "- **Ganancia de informaci贸n**:  \n",
    "  $$IG = \\text{Entrop铆a}_{\\text{Padre}} - \\sum \\left( \\frac{n_j}{n} \\cdot \\text{Entrop铆a}_{\\text{Hijo}_j} \\right)$$\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- F谩cil de entender e interpretar\n",
    "- Requiere poca preparaci贸n de datos\n",
    "- Maneja naturalmente variables categ贸ricas y num茅ricas\n",
    "- Puede capturar relaciones no lineales\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Propenso al sobreajuste, especialmente con 谩rboles profundos\n",
    "- Inestable (peque帽os cambios en los datos pueden generar 谩rboles muy diferentes)\n",
    "- Sesgado hacia caracter铆sticas con muchas categor铆as\n",
    "- Dificultad para capturar relaciones lineales simples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424ec94",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.5 **Random Forest para clasificaci贸n** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Random Forest para clasificaci贸n es un algoritmo de ensamble que combina m煤ltiples 谩rboles de decisi贸n para crear un modelo m谩s robusto y preciso.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**C贸mo funciona**</span>\n",
    "1. Crea m煤ltiples 谩rboles de decisi贸n utilizando bootstrap aggregating (bagging):\n",
    "   - Para cada 谩rbol, selecciona una muestra aleatoria con reemplazo del conjunto de datos original\n",
    "   - Para cada divisi贸n, considera solo un subconjunto aleatorio de caracter铆sticas\n",
    "2. La clase predicha es la moda (voto mayoritario) de las clases predichas por los 谩rboles individuales\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matem谩ticos**</span>\n",
    "- La diversidad entre los 谩rboles se logra mediante:\n",
    "  - Bootstrap sampling: Cada 谩rbol se entrena con una muestra diferente\n",
    "  - Feature randomness: En cada nodo, solo se considera un subconjunto aleatorio de caracter铆sticas\n",
    "- Para m caracter铆sticas, t铆picamente se seleccionan m caracter铆sticas en cada divisi贸n para clasificaci贸n\n",
    "- La probabilidad de cada clase se puede estimar como la proporci贸n de 谩rboles que votaron por esa clase\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Mayor precisi贸n que los 谩rboles individuales\n",
    "- Reduce significativamente el sobreajuste\n",
    "- Robusto a outliers y ruido\n",
    "- Proporciona medidas de importancia de caracter铆sticas\n",
    "- Funciona bien sin mucho ajuste de hiperpar谩metros\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Menos interpretable que un 谩rbol individual\n",
    "- Computacionalmente intensivo\n",
    "- Puede ser lento para predicciones en tiempo real con muchos 谩rboles\n",
    "- Sigue siendo sensible al desbalance de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35dee0",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.6 **Gradient Boosting para clasificaci贸n** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Gradient Boosting es un algoritmo de ensamble que construye modelos secuencialmente, donde cada nuevo modelo intenta corregir los errores del conjunto de modelos anteriores.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**C贸mo funciona**</span>\n",
    "1. Inicia con un modelo simple (generalmente un 谩rbol poco profundo)\n",
    "2. Calcula los errores residuales de este modelo\n",
    "3. Entrena un nuevo modelo para predecir estos residuales\n",
    "4. A帽ade este nuevo modelo al ensamble con un peso determinado\n",
    "5. Repite los pasos 2-4 hasta alcanzar un n煤mero espec铆fico de modelos o hasta que los errores no mejoren significativamente\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matem谩ticos**</span>\n",
    "- Optimiza una funci贸n de p茅rdida (como entrop铆a cruzada para clasificaci贸n)\n",
    "- Utiliza descenso de gradiente para minimizar esta funci贸n\n",
    "- Cada nuevo modelo se ajusta para reducir el gradiente negativo de la funci贸n de p茅rdida\n",
    "- Aplica un factor de aprendizaje (learning rate) para controlar la contribuci贸n de cada modelo\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Variantes importantes**</span>\n",
    "- **XGBoost**: Implementaci贸n optimizada y eficiente que incluye regularizaci贸n\n",
    "- **LightGBM**: Enfoque en eficiencia con divisi贸n basada en histogramas\n",
    "- **CatBoost**: Maneja bien variables categ贸ricas y reduce el sobreajuste\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Generalmente ofrece mejor precisi贸n que Random Forest\n",
    "- Maneja bien diferentes tipos de datos\n",
    "- Proporciona medidas de importancia de caracter铆sticas\n",
    "- Vers谩til para diferentes funciones de p茅rdida\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Puede sobreajustar si no se configuran bien los par谩metros\n",
    "- M谩s sensible a valores at铆picos que Random Forest\n",
    "- M谩s par谩metros para ajustar\n",
    "- Secuencial por naturaleza (dif铆cil de paralelizar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95897ff9",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.7 **Redes neuronales para clasificaci贸n** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Las redes neuronales son modelos inspirados en la estructura del cerebro humano, compuestos por capas de neuronas interconectadas que pueden aprender representaciones complejas de los datos.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**C贸mo funciona**</span>\n",
    "1. La red consta de:\n",
    "   - Capa de entrada: Recibe las caracter铆sticas\n",
    "   - Capas ocultas: Transforman los datos a trav茅s de funciones de activaci贸n\n",
    "   - Capa de salida: Produce las probabilidades para cada clase\n",
    "2. Durante el entrenamiento:\n",
    "   - Forward pass: Los datos pasan a trav茅s de la red para obtener predicciones\n",
    "   - C谩lculo del error: Se comparan las predicciones con las etiquetas reales\n",
    "   - Backpropagation: Se actualizan los pesos para minimizar el error\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matem谩ticos**</span>\n",
    "- Cada neurona aplica una transformaci贸n lineal seguida de una funci贸n de activaci贸n no lineal\n",
    "- Las funciones de activaci贸n comunes incluyen ReLU, sigmoide y tanh\n",
    "- Para clasificaci贸n multiclase, la capa de salida utiliza la funci贸n softmax\n",
    "- La funci贸n de p茅rdida t铆pica es la entrop铆a cruzada\n",
    "- Los pesos se actualizan mediante descenso de gradiente y backpropagation\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Puede capturar relaciones extremadamente complejas y no lineales\n",
    "- Escalable a grandes conjuntos de datos\n",
    "- Excelente para problemas donde las caracter铆sticas tienen jerarqu铆as o patrones\n",
    "- Vers谩til para diferentes tipos de datos (im谩genes, texto, series temporales)\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Requiere gran cantidad de datos para generalizar bien\n",
    "- Computacionalmente intensivo para entrenar\n",
    "- Dif铆cil de interpretar (\"caja negra\")\n",
    "- Muchos hiperpar谩metros para ajustar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f681d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.8 **Naive Bayes** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Naive Bayes es un clasificador probabil铆stico basado en el teorema de Bayes con la suposici贸n \"ingenua\" de que las caracter铆sticas son independientes entre s铆 dado el valor de la clase.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**C贸mo funciona**</span>\n",
    "1. Utiliza el **teorema de Bayes**:  \n",
    "   $$P(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot P(Y)}{P(X)}$$\n",
    "2. Asume **independencia condicional entre caracter铆sticas**:  \n",
    "   $$P(X \\mid Y) = P(X_1 \\mid Y) \\cdot P(X_2 \\mid Y) \\cdot \\dots \\cdot P(X_n \\mid Y)$$\n",
    "3. Clasifica eligiendo la clase con **mayor probabilidad posterior**\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matem谩ticos**</span>\n",
    "- **Teorema de Bayes**:  \n",
    "  $$P(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot P(Y)}{P(X)}$$\n",
    "- La clase predicha es:  \n",
    "  $$y = \\arg\\max_y \\left[ P(y) \\cdot \\prod P(x_i \\mid y) \\right]$$\n",
    "- \\( P(X) \\) es constante para todas las clases, por lo que se puede omitir en la maximizaci贸n\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Variantes principales**</span>\n",
    "- **Gaussiano**: Asume que los valores de caracter铆sticas siguen una distribuci贸n normal\n",
    "- **Multinomial**: Adecuado para datos de conteo (frecuencias)\n",
    "- **Bernoulli**: Para caracter铆sticas binarias/booleanas\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Simple y r谩pido de entrenar\n",
    "- Funciona bien con conjuntos de datos peque帽os\n",
    "- Eficiente con alta dimensionalidad\n",
    "- Maneja bien la clasificaci贸n multiclase\n",
    "- No sensible al ruido irrelevante\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- La suposici贸n de independencia rara vez es cierta en la pr谩ctica\n",
    "- Puede ser superado por modelos m谩s sofisticados cuando hay suficientes datos\n",
    "- No captura interacciones entre caracter铆sticas\n",
    "- Puede tener problemas con caracter铆sticas continuas si no siguen la distribuci贸n asumida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe46a31",
   "metadata": {},
   "source": [
    "## <span style=\"color:#c69005\"> 2. C贸mo evaluamos nuestros algoritmos: m茅tricas de evaluaci贸n </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c4d17",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.1 **M茅tricas b谩sicas** </span>\n",
    "\n",
    "- **Exactitud (Accuracy)**: Proporci贸n de predicciones correctas.\n",
    "- **Precisi贸n (Precision)**: De los casos que predijo como positivos, cu谩ntos eran realmente positivos.\n",
    "- **Recall (Sensibilidad)**: De todos los casos realmente positivos, cu谩ntos identific贸 correctamente.\n",
    "- **F1-Score**: Media arm贸nica entre precisi贸n y recall.\n",
    "- **Especificidad**: De todos los casos realmente negativos, cu谩ntos identific贸 correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e415449",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.1 **Matriz de confusi贸n** </span>\n",
    "- **Verdaderos Positivos (TP)**: Casos positivos correctamente identificados.\n",
    "- **Falsos Positivos (FP)**: Casos negativos incorrectamente clasificados como positivos.\n",
    "- **Verdaderos Negativos (TN)**: Casos negativos correctamente identificados.\n",
    "- **Falsos Negativos (FN)**: Casos positivos incorrectamente clasificados como negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959d75a",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.3 **M茅tricas avanzadas** </span>\n",
    "\n",
    "- **Curva ROC**: Representa la tasa de verdaderos positivos vs. falsos positivos.\n",
    "- **rea Bajo la Curva (AUC)**: Medida del rendimiento del clasificador con diferentes umbrales.\n",
    "- **Log Loss**: Eval煤a la calidad de las probabilidades predichas.\n",
    "- **Coeficiente Kappa de Cohen**: Mide la concordancia entre las etiquetas predichas y reales, teniendo en cuenta la posibilidad de acuerdo por azar."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

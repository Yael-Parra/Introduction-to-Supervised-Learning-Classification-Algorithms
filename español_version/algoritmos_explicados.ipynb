{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d2ac5a",
   "metadata": {},
   "source": [
    "# <span style=\"color:#f6f794\"> Algoritmos de clasificación </span> 📚 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4679eb",
   "metadata": {},
   "source": [
    "## <span style=\"color:#c69005\"> 1. Cómo funcionan y en qué se diferencian </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1814458",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.1 **Regresión logística** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "A pesar de su nombre, la regresión logística es un algoritmo de clasificación que estima la probabilidad de que una instancia pertenezca a una clase particular.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Cómo funciona**</span>\n",
    "1. Calcula una combinación lineal de las características (similar a regresión lineal)\n",
    "2. Aplica la función logística (sigmoide) para transformar el resultado en una probabilidad entre 0 y 1\n",
    "3. Asigna la clase según un umbral (típicamente 0.5)\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matemáticos**</span>\n",
    "  - **Función logísitica**:\n",
    "  $$\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad \\text{donde } z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$$\n",
    "  - **Probabilidad**:  \n",
    "    $$p(y=1 \\mid \\mathbf{x}) = \\sigma(z)$$\n",
    "  - **Función de costo** (log-loss o entropía cruzada binaria):  \n",
    "    $$J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]$$\n",
    "  - **Optimización**:  \n",
    "    Típicamente mediante **descenso de gradiente**:\n",
    "    $$\\beta_j := \\beta_j - \\alpha \\frac{\\partial J(\\beta)}{\\partial \\beta_j}$$\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Proporciona probabilidades, no solo predicciones de clase\n",
    "- Relativamente simple y eficiente\n",
    "- Los coeficientes son interpretables como log-odds\n",
    "- Funciona bien con grandes conjuntos de datos\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Asume relación lineal entre variables y el logaritmo de odds\n",
    "- No maneja bien relaciones no lineales sin transformación de características\n",
    "- Puede tener problemas con clases desbalanceadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a92fa",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.2 **K Nearest Neighbors** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "KNN es un algoritmo simple pero efectivo que clasifica una nueva instancia según la clase mayoritaria de sus k vecinos más cercanos en el espacio de características.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Cómo funciona**</span>\n",
    "1. Almacena todos los datos de entrenamiento\n",
    "2. Para una nueva instancia:\n",
    "   - Calcula la distancia a todas las instancias de entrenamiento\n",
    "   - Identifica los k vecinos más cercanos\n",
    "   - Asigna la clase por voto mayoritario entre estos vecinos\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matemáticos**</span>\n",
    "- Típicamente utiliza **distancia euclidiana**:  \n",
    "    $$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum (x_i - y_i)^2}$$\n",
    "- Otras métricas incluyen distancia de Manhattan, Minkowski o similitud del coseno.\n",
    "- La predicción es:  \n",
    "    $$\\hat{y} = \\arg\\max \\left( \\sum \\mathbb{I}(y_i = c) \\right)$$  \n",
    "    donde \\( \\mathbb{I} \\) es una función indicadora.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Simple y fácil de entender\n",
    "- No hace suposiciones sobre la distribución de los datos\n",
    "- Funciona bien con clases bien separadas\n",
    "- Naturalmente maneja clasificación multiclase\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Computacionalmente intensivo para grandes conjuntos de datos\n",
    "- Sensible a características irrelevantes y a la escala\n",
    "- Requiere selección cuidadosa del parámetro k\n",
    "- No produce un modelo explícito (aprendizaje \"perezoso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352eca28",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.3 **SVM (Máquinas de vectores de soporte)s** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "SVM busca encontrar el hiperplano óptimo que maximiza el margen entre clases en el espacio de características, potencialmente transformado por una función kernel.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Cómo funciona**</span>\n",
    "1. Encuentra el hiperplano que maximiza la distancia entre las clases\n",
    "2. Los puntos más cercanos al hiperplano se llaman \"vectores de soporte\"\n",
    "3. Para problemas no linealmente separables:\n",
    "   - Utiliza una función kernel para mapear los datos a un espacio de mayor dimensión\n",
    "   - Encuentra un hiperplano separador en ese espacio\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matemáticos**</span>\n",
    "- Busca maximizar el margen:  \n",
    "  $$\\frac{2}{\\|\\mathbf{w}\\|}$$\n",
    "- Con restricción:  \n",
    "  $$y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\text{para todos los puntos}$$\n",
    "- Parámetro \\( C \\) controla el equilibrio entre maximizar el margen y minimizar el error\n",
    "- Kernels comunes: lineal, polinómico, RBF (base radial), sigmoide\n",
    "\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Efectivo en espacios de alta dimensionalidad\n",
    "- Robusto cuando hay una clara separación entre clases\n",
    "- Usa solo un subconjunto de puntos (vectores de soporte)\n",
    "- Versátil a través de diferentes funciones kernel\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Selección de parámetros y kernel crítica\n",
    "- No escalable a grandes conjuntos de datos\n",
    "- No proporciona directamente probabilidades\n",
    "- Sensible a ruido y sobreajuste con ciertos kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae864c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.4 **Árboles de decisión para clasificación** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Los árboles de decisión para clasificación crean un modelo con estructura de árbol donde cada nodo interno representa una prueba sobre una característica, cada rama representa el resultado de esa prueba, y cada hoja representa una clase.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Cómo funciona**</span>\n",
    "1. Comienza con todos los datos en el nodo raíz\n",
    "2. Para cada división potencial:\n",
    "   - Calcula una medida de impureza (Gini o entropía)\n",
    "   - Selecciona la división que maximiza la reducción en impureza\n",
    "3. Repite recursivamente en los nodos hijos hasta alcanzar un criterio de parada\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matemáticos**</span>\n",
    "- **Índice Gini**: Mide la probabilidad de clasificar incorrectamente un elemento  \n",
    "  $$\\text{Gini} = 1 - \\sum p_i^2$$  \n",
    "  donde \\( p_i \\) es la proporción de la clase \\( i \\)\n",
    "- **Entropía**: Mide la incertidumbre o desorden en los datos  \n",
    "  $$\\text{Entropía} = -\\sum p_i \\log_2(p_i)$$\n",
    "- **Ganancia de información**:  \n",
    "  $$IG = \\text{Entropía}_{\\text{Padre}} - \\sum \\left( \\frac{n_j}{n} \\cdot \\text{Entropía}_{\\text{Hijo}_j} \\right)$$\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Fácil de entender e interpretar\n",
    "- Requiere poca preparación de datos\n",
    "- Maneja naturalmente variables categóricas y numéricas\n",
    "- Puede capturar relaciones no lineales\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Propenso al sobreajuste, especialmente con árboles profundos\n",
    "- Inestable (pequeños cambios en los datos pueden generar árboles muy diferentes)\n",
    "- Sesgado hacia características con muchas categorías\n",
    "- Dificultad para capturar relaciones lineales simples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424ec94",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.5 **Random Forest para clasificación** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Random Forest para clasificación es un algoritmo de ensamble que combina múltiples árboles de decisión para crear un modelo más robusto y preciso.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Cómo funciona**</span>\n",
    "1. Crea múltiples árboles de decisión utilizando bootstrap aggregating (bagging):\n",
    "   - Para cada árbol, selecciona una muestra aleatoria con reemplazo del conjunto de datos original\n",
    "   - Para cada división, considera solo un subconjunto aleatorio de características\n",
    "2. La clase predicha es la moda (voto mayoritario) de las clases predichas por los árboles individuales\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matemáticos**</span>\n",
    "- La diversidad entre los árboles se logra mediante:\n",
    "  - Bootstrap sampling: Cada árbol se entrena con una muestra diferente\n",
    "  - Feature randomness: En cada nodo, solo se considera un subconjunto aleatorio de características\n",
    "- Para m características, típicamente se seleccionan √m características en cada división para clasificación\n",
    "- La probabilidad de cada clase se puede estimar como la proporción de árboles que votaron por esa clase\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Mayor precisión que los árboles individuales\n",
    "- Reduce significativamente el sobreajuste\n",
    "- Robusto a outliers y ruido\n",
    "- Proporciona medidas de importancia de características\n",
    "- Funciona bien sin mucho ajuste de hiperparámetros\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Menos interpretable que un árbol individual\n",
    "- Computacionalmente intensivo\n",
    "- Puede ser lento para predicciones en tiempo real con muchos árboles\n",
    "- Sigue siendo sensible al desbalance de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35dee0",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.6 **Gradient Boosting para clasificación** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Gradient Boosting es un algoritmo de ensamble que construye modelos secuencialmente, donde cada nuevo modelo intenta corregir los errores del conjunto de modelos anteriores.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Cómo funciona**</span>\n",
    "1. Inicia con un modelo simple (generalmente un árbol poco profundo)\n",
    "2. Calcula los errores residuales de este modelo\n",
    "3. Entrena un nuevo modelo para predecir estos residuales\n",
    "4. Añade este nuevo modelo al ensamble con un peso determinado\n",
    "5. Repite los pasos 2-4 hasta alcanzar un número específico de modelos o hasta que los errores no mejoren significativamente\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matemáticos**</span>\n",
    "- Optimiza una función de pérdida (como entropía cruzada para clasificación)\n",
    "- Utiliza descenso de gradiente para minimizar esta función\n",
    "- Cada nuevo modelo se ajusta para reducir el gradiente negativo de la función de pérdida\n",
    "- Aplica un factor de aprendizaje (learning rate) para controlar la contribución de cada modelo\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Variantes importantes**</span>\n",
    "- **XGBoost**: Implementación optimizada y eficiente que incluye regularización\n",
    "- **LightGBM**: Enfoque en eficiencia con división basada en histogramas\n",
    "- **CatBoost**: Maneja bien variables categóricas y reduce el sobreajuste\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Generalmente ofrece mejor precisión que Random Forest\n",
    "- Maneja bien diferentes tipos de datos\n",
    "- Proporciona medidas de importancia de características\n",
    "- Versátil para diferentes funciones de pérdida\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Puede sobreajustar si no se configuran bien los parámetros\n",
    "- Más sensible a valores atípicos que Random Forest\n",
    "- Más parámetros para ajustar\n",
    "- Secuencial por naturaleza (difícil de paralelizar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95897ff9",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.7 **Redes neuronales para clasificación** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Las redes neuronales son modelos inspirados en la estructura del cerebro humano, compuestos por capas de neuronas interconectadas que pueden aprender representaciones complejas de los datos.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Cómo funciona**</span>\n",
    "1. La red consta de:\n",
    "   - Capa de entrada: Recibe las características\n",
    "   - Capas ocultas: Transforman los datos a través de funciones de activación\n",
    "   - Capa de salida: Produce las probabilidades para cada clase\n",
    "2. Durante el entrenamiento:\n",
    "   - Forward pass: Los datos pasan a través de la red para obtener predicciones\n",
    "   - Cálculo del error: Se comparan las predicciones con las etiquetas reales\n",
    "   - Backpropagation: Se actualizan los pesos para minimizar el error\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matemáticos**</span>\n",
    "- Cada neurona aplica una transformación lineal seguida de una función de activación no lineal\n",
    "- Las funciones de activación comunes incluyen ReLU, sigmoide y tanh\n",
    "- Para clasificación multiclase, la capa de salida utiliza la función softmax\n",
    "- La función de pérdida típica es la entropía cruzada\n",
    "- Los pesos se actualizan mediante descenso de gradiente y backpropagation\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Puede capturar relaciones extremadamente complejas y no lineales\n",
    "- Escalable a grandes conjuntos de datos\n",
    "- Excelente para problemas donde las características tienen jerarquías o patrones\n",
    "- Versátil para diferentes tipos de datos (imágenes, texto, series temporales)\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- Requiere gran cantidad de datos para generalizar bien\n",
    "- Computacionalmente intensivo para entrenar\n",
    "- Difícil de interpretar (\"caja negra\")\n",
    "- Muchos hiperparámetros para ajustar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f681d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 1.8 **Naive Bayes** </span>\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Concepto**</span>\n",
    "\n",
    "Naive Bayes es un clasificador probabilístico basado en el teorema de Bayes con la suposición \"ingenua\" de que las características son independientes entre sí dado el valor de la clase.\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Cómo funciona**</span>\n",
    "1. Utiliza el **teorema de Bayes**:  \n",
    "   $$P(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot P(Y)}{P(X)}$$\n",
    "2. Asume **independencia condicional entre características**:  \n",
    "   $$P(X \\mid Y) = P(X_1 \\mid Y) \\cdot P(X_2 \\mid Y) \\cdot \\dots \\cdot P(X_n \\mid Y)$$\n",
    "3. Clasifica eligiendo la clase con **mayor probabilidad posterior**\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Fundamentos matemáticos**</span>\n",
    "- **Teorema de Bayes**:  \n",
    "  $$P(Y \\mid X) = \\frac{P(X \\mid Y) \\cdot P(Y)}{P(X)}$$\n",
    "- La clase predicha es:  \n",
    "  $$y = \\arg\\max_y \\left[ P(y) \\cdot \\prod P(x_i \\mid y) \\right]$$\n",
    "- \\( P(X) \\) es constante para todas las clases, por lo que se puede omitir en la maximización\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Variantes principales**</span>\n",
    "- **Gaussiano**: Asume que los valores de características siguen una distribución normal\n",
    "- **Multinomial**: Adecuado para datos de conteo (frecuencias)\n",
    "- **Bernoulli**: Para características binarias/booleanas\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Ventajas**</span>\n",
    "- Simple y rápido de entrenar\n",
    "- Funciona bien con conjuntos de datos pequeños\n",
    "- Eficiente con alta dimensionalidad\n",
    "- Maneja bien la clasificación multiclase\n",
    "- No sensible al ruido irrelevante\n",
    "\n",
    "##### <span style=\"color:#f6f794\">**Limitaciones**</span>\n",
    "- La suposición de independencia rara vez es cierta en la práctica\n",
    "- Puede ser superado por modelos más sofisticados cuando hay suficientes datos\n",
    "- No captura interacciones entre características\n",
    "- Puede tener problemas con características continuas si no siguen la distribución asumida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe46a31",
   "metadata": {},
   "source": [
    "## <span style=\"color:#c69005\"> 2. Cómo evaluamos nuestros algoritmos: métricas de evaluación </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c4d17",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.1 **Métricas básicas** </span>\n",
    "\n",
    "- **Exactitud (Accuracy)**: Proporción de predicciones correctas.\n",
    "- **Precisión (Precision)**: De los casos que predijo como positivos, cuántos eran realmente positivos.\n",
    "- **Recall (Sensibilidad)**: De todos los casos realmente positivos, cuántos identificó correctamente.\n",
    "- **F1-Score**: Media armónica entre precisión y recall.\n",
    "- **Especificidad**: De todos los casos realmente negativos, cuántos identificó correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e415449",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.1 **Matriz de confusión** </span>\n",
    "- **Verdaderos Positivos (TP)**: Casos positivos correctamente identificados.\n",
    "- **Falsos Positivos (FP)**: Casos negativos incorrectamente clasificados como positivos.\n",
    "- **Verdaderos Negativos (TN)**: Casos negativos correctamente identificados.\n",
    "- **Falsos Negativos (FN)**: Casos positivos incorrectamente clasificados como negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959d75a",
   "metadata": {},
   "source": [
    "### <span style=\"color:#c69005\"> 2.3 **Métricas avanzadas** </span>\n",
    "\n",
    "- **Curva ROC**: Representa la tasa de verdaderos positivos vs. falsos positivos.\n",
    "- **Área Bajo la Curva (AUC)**: Medida del rendimiento del clasificador con diferentes umbrales.\n",
    "- **Log Loss**: Evalúa la calidad de las probabilidades predichas.\n",
    "- **Coeficiente Kappa de Cohen**: Mide la concordancia entre las etiquetas predichas y reales, teniendo en cuenta la posibilidad de acuerdo por azar."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
